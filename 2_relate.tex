\section{Related Work}
\label{sec:related}
General purpose GPUs have become important processing engines for computationally intensive workloads due to their highly parallel computing architectures.
Linear algebra operations are among the first batch of libraries accelerated with GPUs.
There are various such accelerated libraries, i.e.,  CUBLAS, CULA, MAGMA. The BLAS (Basic Linear Algebra Subprograms) are routines supported by Nvidia \cite{cublas} that provide standard building blocks for performing basic vector and matrix operations. CUBLAS is an implementation of BLAS and provides many solutions for common linear algebra operations. 
However, it does not provide solutions to complex linear algebra problems, such as QR decomposition, LU decomposition and SVD.
CULA consists of commercial hybrid GPU accelerated linear algebra routines\cite{cula} that support high-level linear algebra operations.
Among them, the singular value decomposition employs QR algorithm, which is not an effective algorithm when the matrix size becomes significantly large.
MAGMA aims to achieve high performance and portability across a wide range of multi-core architectures and hybrid systems respectively\cite{magma}.

QR, the most commonly used algorithm for computing singular values and vectors, 
is regarded as a powerful and effective algorithm with high accuracy
and numerical stability\cite{97bookalgebra}. 
Their implementation shows a speedup of up to 8 over the Intel MKL QR implementation \cite{mkl}.
The first SVD algorithm with CUDA programming is implemented by Sheetal et al.\cite{09IPDPSQR}. With parallelized QR iteration algorithm using CUBLAS library on GPU, they achieved a speedup of up to 8 over the Intel MKL QR implementation.
However, the QR-iteration algorithm has its drawback on parallelization due to its heavy data dependency.
The algorithm requires $O(n^3)$ to complete the diagonalization and its decomposition time is intolerable as size of matrices increase.
%Moreover, calling too much CUBLAS kernels also reduces the performance in the GPU design.

Liu et al. use a divide-and-conquer approach to solve SVD on a heterogeneous CPU-GPU system \cite{13CFDC}.
It is almost 7 times faster than CULA QR algorithm executing on the same device M2070, and up to 33 times faster than LAPACK.
However, DC approach is relatively inaccurate during the merging phase, especially when the data scale is huge.
In the worst case, it will require $O(n^3)$ to complete SVD, if the singular values are in a dense distribution\cite{97bookalgebra}.
%This is because the implementation that makes use of pipeline architecture in a heterogeneous architecture does not make full use of resources on GPUs and the frequent data transfer between GPU and CPU dramatically slow down the execution time.

Vedran\cite{14arxivjacobi} presents a hierarchically blocked one-sided Jacobi algorithm for the singular value decomposition on both single and multiple GPU architectures. 
The algorithm maintains high accuracy in singular values and vectors. Due to the speed limitation of the algorithm, even with full optimizations and a high speedup compared to the same algorithm on CPU, the execution time is still more than that of Sheetal's QR implementation.


In \cite{99clustering} Drineas et al. provide a clustered SVD algorithm for large matrices. The algorithm divides a set of $n$ points into $k$ clusters, where $k$ is much less than $n$ on CPU.
It is an approximation algorithm to obtain only one subset of singular values
and vectors. Our BT algorithm can compute the complete SVD, thus is not
directly compared with \cite{99clustering}. 
%They experimentally
%evaluate the accuracy and speed of this algorithm for image matrices, using
%various probability distributions to perform the sampling\cite{01randomizedSVD,01mentoSVD}.

